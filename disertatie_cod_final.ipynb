{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disertatie-cod-final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnG0hOHnsHus",
        "outputId": "c5ae48e8-902c-44f6-96b4-c7c2c46f19d4"
      },
      "source": [
        "#Cell 1 Import datasets and transform data\n",
        "!pip install extra-keras-datasets\n",
        "\n",
        "#Connect your gdrive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "from extra_keras_datasets import emnist\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np \n",
        "from numpy import load\n",
        "\n",
        "def load_gtsrb():\n",
        "  x_train=load('/content/gdrive/MyDrive/datasets/GTSRB/x_train.npy')\n",
        "  x_test=load('/content/gdrive/MyDrive/datasets/GTSRB/x_test.npy')\n",
        "\n",
        "  y_train=load('/content/gdrive/MyDrive/datasets/GTSRB/y_train.npy')\n",
        "  y_test=load('/content/gdrive/MyDrive/datasets/GTSRB/y_test.npy')\n",
        "  return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "def load_bts():\n",
        "  x_train=load('/content/gdrive/MyDrive/datasets/belgian/32/x_train_32.npy')\n",
        "  x_test=load('/content/gdrive/MyDrive/datasets/belgian/32/x_test_32.npy')\n",
        "\n",
        "  y_train=load('/content/gdrive/MyDrive/datasets/belgian/32/y_train_32.npy')\n",
        "  y_test=load('/content/gdrive/MyDrive/datasets/belgian/32/y_test_32.npy')\n",
        "  return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "def load_tsrd():\n",
        "  x_train=load('/content/gdrive/MyDrive/datasets/tsrd/32/x_train_32.npy')\n",
        "  x_test=load('/content/gdrive/MyDrive/datasets/tsrd/32/x_test_32.npy')\n",
        "\n",
        "  y_train=load('/content/gdrive/MyDrive/datasets/tsrd/32/y_train_32.npy')\n",
        "  y_test=load('/content/gdrive/MyDrive/datasets/tsrd/32/y_test_32.npy')\n",
        "  return (x_train, y_train, x_test, y_test)  \n",
        "\n",
        "def load_european():\n",
        "  x_train=load('/content/gdrive/MyDrive/datasets/european/32/x_train_32.npy')\n",
        "  x_test=load('/content/gdrive/MyDrive/datasets/european/32/x_test_32.npy')\n",
        "\n",
        "  y_train=load('/content/gdrive/MyDrive/datasets/european/32y_train_32.npy')\n",
        "  y_test=load('/content/gdrive/MyDrive/datasets/european/32/y_test_32.npy')\n",
        "  return (x_train, y_train, x_test, y_test)  \n",
        "\n",
        "def load_stanford_cars_50():\n",
        "  x_train=load('/content/gdrive/MyDrive/datasets/stanford_cars/x_train.npy')\n",
        "  x_test=load('/content/gdrive/MyDrive/datasets/stanford_cars/x_test.npy')\n",
        "\n",
        "  y_train=load('/content/gdrive/MyDrive/datasets/stanford_cars/y_train.npy')  \n",
        "  y_test=load('/content/gdrive/MyDrive/datasets/stanford_cars/y_test.npy')\n",
        "  return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "def load_stanford_cars_70():\n",
        "  x=load('/content/gdrive/MyDrive/datasets/stanford_cars/csv-version/x.npy')\n",
        "  y=load('/content/gdrive/MyDrive/datasets/stanford_cars/csv-version/y.npy')\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.3,random_state=100)\n",
        "\n",
        "  return (x_train, y_train, x_test, y_test) \n",
        "\n",
        "def load_stanford_cars_brand():\n",
        "  x=load('/content/gdrive/MyDrive/datasets/cars_brand/x.npy')\n",
        "  y=load('/content/gdrive/MyDrive/datasets/cars_brand/y.npy')\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.3,random_state=100)\n",
        "\n",
        "  return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "dataset='gtsrb' # \n",
        "dformat='channels_last'\n",
        "reduced=0\n",
        "bconv=0\n",
        "\n",
        "if dataset=='emnist':\n",
        "    (x_train, y_train), (x_test, y_test) = emnist.load_data(type='balanced')\n",
        "elif dataset=='gtsrb':\n",
        "    (x_train, y_train, x_test, y_test) = load_gtsrb()\n",
        "elif dataset=='bts':\n",
        "    (x_train, y_train, x_test, y_test) = load_bts()\n",
        "elif dataset=='tsrd':\n",
        "    (x_train, y_train, x_test, y_test) = load_tsrd()\n",
        "elif dataset=='european':\n",
        "    (x_train, y_train, x_test, y_test) = load_european()\n",
        "elif dataset=='stanford_cars_50':\n",
        "    (x_train, y_train, x_test, y_test) = load_stanford_cars_50()\n",
        "elif dataset=='stanford_cars_70':\n",
        "    (x_train, y_train, x_test, y_test) = load_stanford_cars_brand()\n",
        "elif dataset=='stanford_cars_brand':\n",
        "    (x_train, y_train, x_test, y_test) = load_stanford_cars_brand()\n",
        "\n",
        "print(\"X_train shape: \" + str(x_train.shape))\n",
        "print(\"Y_train shape: \" + str(y_train.shape))\n",
        "\n",
        "if (np.ndim(x_train)==3):   # Only for EMNIST to keep it compatible cu conv2d kernels in CNNs network\n",
        "    x_train=np.reshape(x_train, [np.shape(x_train)[0],np.shape(x_train)[1],np.shape(x_train)[2], 1]) \n",
        "    x_test=np.reshape(x_test, [np.shape(x_test)[0],np.shape(x_test)[1],np.shape(x_test)[2], 1])\n",
        "\n",
        "if bconv==0: \n",
        "  x_train = x_train.astype('float32') / 255\n",
        "  x_test = x_test.astype('float32') /255\n",
        "  \n",
        "inp_chan=np.shape(x_train)[3]\n",
        "print(\"Number of inpunt channels: \" + str(inp_chan))\n",
        "num_classes=np.max(y_train)+1\n",
        "\n",
        "num_inputs = np.shape(x_test)[1]\n",
        "input_shape=np.shape(x_train)[1:4]\n",
        "\n",
        "# one can choose a lower numbers of training samples (when GPU MEM is overloaded)\n",
        "if reduced>0:\n",
        "    Ntr1=reduced\n",
        "    x_train=x_train[0:Ntr1,:,:,:]\n",
        "    y_train=y_train[0:Ntr1]\n",
        "    print(\"X_train shape after reduction: \" + str(x_train.shape))\n",
        "    print(\"Y_train shape after reduction: \" + str(y_train.shape))\n",
        "\n",
        "\n",
        "yc_train = np_utils.to_categorical(y_train, num_classes)\n",
        "yc_test = np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: extra-keras-datasets in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.1.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->extra-keras-datasets) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->extra-keras-datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->extra-keras-datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->extra-keras-datasets) (1.15.0)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "X_train shape: (39209, 32, 32, 3)\n",
            "Y_train shape: (39209,)\n",
            "Number of inpunt channels: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0z6xFC0y4Dl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54543d8b-0c85-46bc-bf02-664c79d7af7a"
      },
      "source": [
        "# RUN THIS CELL ONLY TO PREPARE DATA FOR MOBILENETv2 \n",
        "# expands to 32x32 and 3 channels \n",
        "# only for EMNIST\n",
        "\n",
        "from skimage import transform\n",
        "# from https://github.com/astorfi/TensorFlow-World/tree/master/docs/tutorials/3-neural_network/autoencoder\n",
        "\n",
        "def resize_batch(imgs):\n",
        "    # A function to resize a batch of MNIST images to (32, 32)\n",
        "    # Args:\n",
        "    #   imgs: a numpy array of size [batch_size, 28 X 28].\n",
        "    # Returns:\n",
        "    #   a numpy array of size [batch_size, 32, 32].\n",
        "    print(\"Imgs pre-reshape : \" + str(imgs.shape))\n",
        "    imgs = imgs.reshape((-1, 28, 28, 1)) #mai adauga o dimensiune\n",
        "    print(\"Imgs after reshape : \" + str(imgs.shape))\n",
        "    print(\"Imgs shape[0] : \" + str(imgs.shape[0]))\n",
        "    resized_imgs = np.zeros((imgs.shape[0], 32, 32, 1))\n",
        "    print(\"Resized_img after reshape : \" + str(resized_imgs.shape))\n",
        "    for i in range(imgs.shape[0]):\n",
        "        resized_imgs[i, ..., 0] = transform.resize(imgs[i, ..., 0], (32, 32))\n",
        "    return resized_imgs\n",
        "\n",
        "\n",
        "x_train=resize_batch(x_train)\n",
        "x_test=resize_batch(x_test)\n",
        "\n",
        "# 3 chans expansion  \n",
        "if inp_chan<3:\n",
        "  \n",
        "  x_train = np.repeat(x_train, 3, -1)\n",
        "  x_test =  np.repeat(x_test, 3, -1)\n",
        "input_shape=np.shape(x_train)[1:4]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imgs pre-reshape : (55000, 28, 28, 1)\n",
            "Imgs after reshape : (55000, 28, 28, 1)\n",
            "Imgs shape[0] : 55000\n",
            "Resized_img after reshape : (55000, 32, 32, 1)\n",
            "Imgs pre-reshape : (18800, 28, 28, 1)\n",
            "Imgs after reshape : (18800, 28, 28, 1)\n",
            "Imgs shape[0] : 18800\n",
            "Resized_img after reshape : (18800, 32, 32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_x6EDfuyplY"
      },
      "source": [
        "# NL_CNN MODEL \n",
        "# Returns a precompiled model with a specific optimizer included \n",
        "#==============================================================================================\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from keras.layers import Conv2D, DepthwiseConv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, SeparableConv2D  # straturi convolutionale si max-pooling \n",
        "from keras.optimizers import RMSprop, SGD, Adadelta, Adam, Nadam\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import scipy.linalg as sclin\n",
        "import time as ti\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "\n",
        "\n",
        "def create_nl_cnn_model(input_shape, num_classes, k=1.5,separ=0, flat=0, width=80, nl=(3,2), add_layer=0):\n",
        "  # Arguments: k - multiplication coefficient \n",
        "  # Structure parameteres \n",
        "  kfil=k\n",
        "  filtre1=width ; filtre2=int(kfil*filtre1) ; filtre3=(kfil*filtre2)  # filters (kernels) per each layer - efic. pe primul \n",
        "  nr_conv=3 # 0, 1, 2 sau 3  (number of convolution layers)\n",
        "  csize1=3; csize2=3 ; csize3=3      # convolution kernel size (square kernel) \n",
        "  psize1=4; psize2=4 ; psize3=4      # pooling size (square)\n",
        "  str1=2; str2=2; str3=2             # stride pooling (downsampling rate) \n",
        "  pad='same'; # padding style ('valid' is also an alternative)\n",
        "  nonlinlayers1=nl[0]  # total of layers (with RELU nonlin) in the first maxpool layer  # De parametrizat asta \n",
        "  nonlinlayers2=nl[1]  # \n",
        "\n",
        "  nonlin_type='relu' # may be other as well 'tanh' 'elu' 'softsign'\n",
        "  bndrop=1 # include BatchNorm inainte de MaxPool si drop(0.3) dupa .. \n",
        "  cvdrop=1 # droput \n",
        "  drop_cv=0.5\n",
        "  \n",
        "  model = Sequential()\n",
        "  # convolution layer1  ==========================================================================\n",
        "  # Initially first layer was always a Conv2D one\n",
        "  if separ==1:\n",
        "    model.add( SeparableConv2D(filtre1, padding=pad, kernel_size=(csize1, csize1), input_shape=input_shape) )\n",
        "  elif separ==0: \n",
        "    model.add( Conv2D(filtre1, padding=pad, kernel_size=(csize1, csize1), input_shape=input_shape) )\n",
        "\n",
        "  # next are the additional layers \n",
        "  for nl in range(nonlinlayers1-1):\n",
        "    model.add(Activation(nonlin_type))  # Activ NL-CNN-1\n",
        "    if separ==1:\n",
        "      model.add(SeparableConv2D(filtre1, padding=pad, kernel_size=(csize1, csize1) ) ) # Activ NL-CNN-2\n",
        "    elif separ==0:\n",
        "      model.add(Conv2D(filtre1, padding=pad, kernel_size=(csize1, csize1)) ) # Activ NL-CNN-2\n",
        "  #  MaxPool in the end of the module \n",
        "  if bndrop==1:\n",
        "    model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(psize1, psize1),strides=(str1,str1),padding=pad))\n",
        "  if cvdrop==1:\n",
        "    model.add(Dropout(drop_cv))\n",
        "  \n",
        "  # NL LAYER 2 =======================================================================================================\n",
        " \n",
        "  if separ==1:\n",
        "    model.add(SeparableConv2D(filtre2, padding=pad, kernel_size=(csize2, csize2)) )\n",
        "  elif separ==0:\n",
        "    model.add(Conv2D(filtre2, padding=pad, kernel_size=(csize2, csize2)) )\n",
        "  # aici se adauga un neliniar \n",
        "    \n",
        "  #=========== unul extra NL=2 pe strat 2 =====================\n",
        "  for nl in range(nonlinlayers2-1):\n",
        "    model.add(Activation(nonlin_type))  # Activ NL-CNN-1\n",
        "    if separ==1:\n",
        "        model.add(SeparableConv2D(filtre2, padding=pad, kernel_size=(csize2, csize2)) ) # Activ NL-CNN-2\n",
        "    elif separ==0:\n",
        "        model.add(Conv2D(filtre2, padding=pad, kernel_size=(csize2, csize2)) ) # Activ NL-CNN-2\n",
        "        \n",
        "  # OUTPUT OF LAYER 2 (MAX-POOL)\n",
        "  if bndrop==1:\n",
        "      model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(psize2, psize2),strides=(str2,str2),padding=pad))\n",
        "  if cvdrop==1:\n",
        "      model.add(Dropout(drop_cv))\n",
        "  #-------------------------------------------------------------------------------------------\n",
        "  # LAYER 3 \n",
        "      \n",
        "  if separ==1:\n",
        "      model.add(SeparableConv2D(filtre3, padding=pad, kernel_size=(csize3, csize3)) )  # SeparableConv\n",
        "  elif separ==0:\n",
        "      model.add(Conv2D(filtre3, padding=pad, kernel_size=(csize3, csize3)) ) # Activ NL-CNN-2\n",
        "  # OUTPUT OF LAYER 3 \n",
        "  if bndrop==1:\n",
        "      model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(psize3, psize3),strides=(str3,str3),padding=pad))\n",
        "  if cvdrop==1:\n",
        "      model.add(Dropout(drop_cv))\n",
        "  #------------------- \n",
        "  # \n",
        "  # LAYER 4  (only if requested - for large images ?? )\n",
        "  if add_layer==1:    \n",
        "    if separ==1:\n",
        "      model.add(SeparableConv2D(1.2*filtre3, padding=pad, kernel_size=(csize3, csize3)) )  # SeparableConv\n",
        "    elif separ==0:\n",
        "      model.add(Conv2D(1.2*filtre3, padding=pad, kernel_size=(csize3, csize3)) ) # Activ NL-CNN-2\n",
        "    # OUTPUT OF LAYER 4\n",
        "    if bndrop==1:\n",
        "      model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(psize3, psize3),strides=(str3,str3),padding=pad))\n",
        "    if cvdrop==1:\n",
        "      model.add(Dropout(drop_cv))\n",
        "  #========================================================================================\n",
        "  # INPUT TO DENSE LAYER (FLATTEN - more data can overfit / GLOBAL - less data - may be a good choice ) \n",
        "  if flat==1:\n",
        "      model.add(Flatten())  # \n",
        "  elif flat==0:\n",
        "      model.add(GlobalAveragePooling2D()) # Global average \n",
        "   \n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  # END OF MODEL DESCRIPTION \n",
        "  # ------------------ COMPILE THE MODEL\n",
        "  myopt = Adam()\n",
        "  #myopt = Nadam()\n",
        "  if separ==1:\n",
        "    myopt = RMSprop(lr=0.01) \n",
        "    #myopt = Adam(lr=0.05)\n",
        "\n",
        "  # --------------------------   LOSS function  ------------------------------------\n",
        "  my_loss='categorical_crossentropy'\n",
        "  model.compile(loss=my_loss, \n",
        "              optimizer=myopt,   \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "  #######################LB-CNN-MODEL########################################\n",
        "\n",
        "def create_lb_cnn_model(input_shape, num_classes, type_convolution = 2, filtre=(100,100,100), numar_straturi=1, neuroni_ascunsi=(0,0)):\n",
        "#--------------  Output layer MLP  (nhid1=nhid2=0 recommended for low complexity) \n",
        "  nhid1 = neuroni_ascunsi[0] # hidden-1 neurons (put 0 if nhid2=0, or a desired value)\n",
        "  nhid2 = neuroni_ascunsi[1] # hidden-2 neurons (take 0 for 0 or 1 hidden layer)\n",
        "\n",
        "#\n",
        "# ----------------- CONV expander layers (up to 3) ------------------------------------------------------------\n",
        "  nr_conv=numar_straturi # 0, 1, 2 sau 3  (number of convolution layers)\n",
        "  filtre1=filtre[0] ; filtre2=filtre[1] ; filtre3=filtre[2]  # filters (kernels) per each layer \n",
        "  csize1=3 ; csize2=3 ; csize3=3      # convolution kernel size (square kernel) \n",
        "  psize1=4; psize2=4 ; psize3=2      # pooling size (square)\n",
        "  str1=2; str2=2; str3=2             # stride pooling (downsampling rate) \n",
        "  pad='same'; # padding style ('valid' is also an alternative)\n",
        "  type_conv=type_convolution # 1='depth_wise' or 2='normal' \n",
        "# ------------------- Optimizer -----------------------------------------------------------------\n",
        "  #myopt = SGD(learning_rate=0.01, decay=1e-6, momentum=.9, nesterov=True)\n",
        "  #myopt =Adadelta(learning_rate=.1)  # implicit are lr=1 # cum influenteaza valoarea procesul de antrenare ?? \n",
        "  myopt = RMSprop(learning_rate=0.0005) \n",
        "  #myopt = Adam(learning_rate=0.001)\n",
        "  #myopt = Adam()\n",
        "# --------------------------   LOSS function  ------------------------------------\n",
        "  #my_loss='mean_squared_error'  \n",
        "  #my_loss='mean_absolute_error'\n",
        "  my_loss='categorical_crossentropy'\n",
        "\n",
        "#-------------------------- MODEL DESCRIPTION ------------------------------\n",
        "  model = Sequential()\n",
        "\n",
        "  if nr_conv>=1:\n",
        "      if type_conv==2:\n",
        "          model.add(Conv2D(filtre1, padding=pad, kernel_size=(csize1, csize1), input_shape=input_shape))\n",
        "      elif type_conv==1:\n",
        "          model.add(DepthwiseConv2D(kernel_size=csize2, padding=pad, input_shape=input_shape, depth_multiplier=filtre1, use_bias=False))\n",
        "      #model.add(Activation('relu'))\n",
        "      model.add(MaxPooling2D(pool_size=(psize1, psize1),strides=(str1,str1),padding=pad))\n",
        "      #model.add(Activation('relu'))\n",
        "      if nr_conv>=2:\n",
        "          if type_conv==2:\n",
        "              model.add(Conv2D(filtre2, padding=pad, kernel_size=(csize2, csize2)) )\n",
        "          elif type_conv==1:\n",
        "              model.add(DepthwiseConv2D(kernel_size=csize2, padding=pad, depth_multiplier=filtre2, use_bias=False))\n",
        "          model.add(Activation('relu'))\n",
        "          model.add(MaxPooling2D(pool_size=(psize2, psize2),strides=(str2,str2),padding=pad))\n",
        "          #model.add(Activation('relu'))\n",
        "          if nr_conv==3:\n",
        "              if type_conv==2:\n",
        "                  model.add(Conv2D(filtre3, padding=pad, kernel_size=(csize3, csize3)) )\n",
        "              elif type_conv==1:\n",
        "                  model.add(DepthwiseConv2D(kernel_size=csize3, padding=pad, depth_multiplier=filtre3, use_bias=False))\n",
        "              #model.add(Activation('relu'))\n",
        "              model.add(MaxPooling2D(pool_size=(psize3, psize3),strides=(str3,str3),padding=pad))\n",
        "              #model.add(Activation('relu'))\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(Flatten())\n",
        "      #model.add(Activation('relu'))\n",
        "      #model.add(Dropout(0.25))\n",
        "  elif nr_conv==0:\n",
        "      model.add(Flatten(input_shape=input_shape))\n",
        "# ---- first fc hidden layer  \n",
        "  if nhid1>0:\n",
        "      model.add(Dense(nhid1, activation='relu'))\n",
        "      #model.add(Dropout(0.5))\n",
        "# ---- second fc hidden layer \n",
        "  if nhid2>0:\n",
        "      model.add(Dense(nhid2, activation='relu'))\n",
        "  #   model.add(Dropout(0.2))\n",
        "#   output layer \n",
        "  if (nhid1+nhid2)==0:\n",
        "      model.add(Dense(num_classes, activation='softmax',input_shape=(num_inputs,)))\n",
        "  else: \n",
        "      model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# --- MODEL COMPILE --------------------------------------------------------\n",
        "\n",
        "  model.compile(loss=my_loss, \n",
        "              optimizer=myopt,   # se poate alege oricare dintre obiectele optimizer definite mai sus \n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_model_mobilenetv2(input_shape, num_classes, alpha_value=0.75):\n",
        "  pretrained_model = tf.keras.applications.MobileNetV2(alpha=alpha_value, input_shape=[input_shape[0], input_shape[1], 3], include_top=False)\n",
        "  pretrained_model.trainable = True   \n",
        "  # True - all weights are trained; False: only the output layer is trained \n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    #tf.keras.layers.Dense(1000,activation='relu'),  # Numai daca se doreste un strat dens suplimentar \n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  myopt = Adam()\n",
        "  #myopt = Nadam()\n",
        "  # --------------------------   LOSS function  ------------------------------------\n",
        "  my_loss='categorical_crossentropy'\n",
        "  model.compile(loss=my_loss, \n",
        "              optimizer=myopt,   \n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_model_nasnetmobile50(input_shape, num_classes):\n",
        "  pretrained_model = tf.keras.applications.NASNetMobile(include_top=False, input_shape=[input_shape[0], input_shape[1], 3], weights=None)\n",
        "  pretrained_model.load_weights('/content/gdrive/MyDrive/datasets/NASNet-mobile-no-top.h5')\n",
        "  pretrained_model.trainable = True\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    #tf.keras.layers.Dense(1000,activation='relu'),  # Numai daca se doreste un strat dens suplimentar \n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  myopt = Adam()\n",
        "  #myopt = Nadam()\n",
        "  # --------------------------   LOSS function  ------------------------------------\n",
        "  my_loss='categorical_crossentropy'\n",
        "  model.compile(loss=my_loss, \n",
        "              optimizer=myopt,   \n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_resnet50_model(input_shape, num_classes):\n",
        "  pretrained_model = tf.keras.applications.ResNet50(include_top=False, input_shape=[input_shape[0], input_shape[1], 3])\n",
        "  pretrained_model.trainable = True\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    #tf.keras.layers.Dense(1000,activation='relu'),  # Numai daca se doreste un strat dens suplimentar \n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  myopt = Adam()\n",
        "  #myopt = Nadam()\n",
        "  # --------------------------   LOSS function  ------------------------------------\n",
        "  my_loss='categorical_crossentropy'\n",
        "  model.compile(loss=my_loss, \n",
        "              optimizer=myopt,   \n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_densenet121_model(input_shape, num_classes):\n",
        "  pretrained_model = tf.keras.applications.DenseNet121(include_top=False, input_shape=[input_shape[0], input_shape[1], 3])\n",
        "  pretrained_model.trainable = True\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    #tf.keras.layers.Dense(1000,activation='relu'),  # Numai daca se doreste un strat dens suplimentar \n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  myopt = Adam()\n",
        "  #myopt = Nadam()\n",
        "  # --------------------------   LOSS function  ------------------------------------\n",
        "  my_loss='categorical_crossentropy'\n",
        "  model.compile(loss=my_loss, \n",
        "              optimizer=myopt,   \n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "def create_effnetb0_model(input_shape, num_classes):\n",
        "  pretrained_model = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=[input_shape[0], input_shape[1], 3])\n",
        "  pretrained_model.trainable = True\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    #tf.keras.layers.Dense(1000,activation='relu'),  # Numai daca se doreste un strat dens suplimentar \n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  myopt = Adam()\n",
        "  #myopt = Nadam()\n",
        "  # --------------------------   LOSS function  ------------------------------------\n",
        "  my_loss='categorical_crossentropy'\n",
        "  model.compile(loss=my_loss, \n",
        "              optimizer=myopt,   \n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_effnetb1_model(input_shape, num_classes):\n",
        "  pretrained_model = tf.keras.applications.EfficientNetB1(include_top=False, input_shape=[input_shape[0], input_shape[1], 3])\n",
        "  pretrained_model.trainable = True\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    #tf.keras.layers.Dense(1000,activation='relu'),  # Numai daca se doreste un strat dens suplimentar \n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "  myopt = Adam()\n",
        "  #myopt = Nadam()\n",
        "  # --------------------------   LOSS function  ------------------------------------\n",
        "  my_loss='categorical_crossentropy'\n",
        "  model.compile(loss=my_loss, \n",
        "              optimizer=myopt,   \n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "    \n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMXegrJ92UI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d0cc35-adca-4639-852d-c678478965c4"
      },
      "source": [
        "'''\n",
        "BCONV MODULE \n",
        "This module loads dataset (one of MNIST, CIFAR10 or Fashion-MNIST)\n",
        "# Copyright Radu & Ioana DOGARU - radu_d@ieee.org , Aug. 13 2019 \n",
        "# More details in paper \n",
        "# R. Dogaru and Ioana Dogaru, \"BCONV-ELM: Binary Weights Convolutional \n",
        "# Neural Network Simulator based on Keras/Tensorflow for Low Complexity \n",
        "# Implementations\", in Proceedings ISEEE 2019, in Press. \n",
        "# Please cite the above paper if you find this code useful \n",
        "'''\n",
        "\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import scipy.linalg as sclin\n",
        "import numpy as np\n",
        "import time as ti\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "'''\n",
        "\n",
        "Uses K.backend methods to implement (GPU/CPU) a preprocessing \n",
        "layer with up to 2 convolutional layers (each with abs() \n",
        "value nonlinearity and pooling)\n",
        "Convolution kernels are BINARY and RANDOM !\n",
        "THIS MODULE WOULD GENERATE out_train out_test \n",
        "AS THE OUTPUT OF A PREPROCESSING NONLINEAR LAYER \n",
        "WITH UP TO 2 CONVOLUTIONAL LAYERS AND x_train x_test tuple at the input. \n",
        "'''\n",
        "\n",
        "#------------------------ Algo parameters ----------\n",
        "########################################################\n",
        "\n",
        "# convolutional layers \n",
        "typ_con=2 #  1 = depthwise (); 2 = normal (as usually used in Keras CNNs)\n",
        "filtre1=10; filtre2=16 # if filtre2=0 only the first layer is implemented  \n",
        "ksize1=3; ksize2=3    # kernel size on layers 1 & 2 \n",
        "pool=(2,2); pool2=(3,3)  # pool size on layers 1 & 2 \n",
        "strp1=2; strp2=2 # strides on pooling layers 1 & 2 \n",
        "strc1=1; strc2=1 # strides on conv layers 1 & 2 \n",
        "pmode='max'  # pooling mode (alternative is  'average', lower performance) \n",
        "pad='same' # 'same' (alternative is  'valid' ) results in less outputs but performance may decrease\n",
        "reduced = 0 # 0-full; >0 = reduced set of training samples (using all samples - choose 0)\n",
        "use_stored = 0 # use bk wher bk=(ker,ker2) are best kernels (saved after run in console mode)\n",
        "# choose 0 above for loop tuning (then run all cells 5-10 trials saving bk=(ker,ker2) for any improvement in ELM0 accuracy)\n",
        "# choose 1 above after tuning then run all cells (now ELM is not active)\n",
        "epoci=1 # epochs in the MLP (use 1 in the ELM loop tuning phase with Run ALL)\n",
        "\n",
        "print('Dataset is:', dataset)\n",
        "print('Convolution type is (1-depthwise, 2-classic)',typ_con)\n",
        "print('Layer 1: filters:',filtre1,' kernel size:',ksize1,'pooling size:',pool,'strides pooling:',strp1)\n",
        "print('Layer 1: filters:',filtre2,' kernel size:',ksize2,'pooling size:',pool2,'strides pooling:',strp2)\n",
        "print('Pooling mode is:',pmode,'  Padding is: ',pad)\n",
        "\n",
        "\n",
        "#Dataset loading\n",
        "\n",
        "print(\"BCONV-ELM - \" + dataset + str(x_train.shape))\n",
        "\n",
        "# definitie primul strat de convolutii (liniare)\n",
        "def convlayer(inlay,ker_,pool,strp,strc,typ_con,pad,pmode):\n",
        "# inlay este intrarea (multicanal si organizata [samples, x_size, y_size, channells])\n",
        "# ker_ este variabila kernel organizata in forma [x_size, x_sixe, channels, filtre ]\n",
        "    dformat='channels_last'\n",
        "    if typ_con==1:\n",
        "        out_=K.depthwise_conv2d(inlay, ker_, strides=(1, 1), padding=pad, data_format=dformat)\n",
        "    elif typ_con==2:\n",
        "        out_=K.conv2d(inlay, ker_, strides=(1, 1), padding=pad, data_format=dformat)\n",
        "        \n",
        "    nout_=K.pool2d(out_,pool, strides=(strp, strp), padding=pad, data_format=dformat ,pool_mode=pmode)\n",
        "    nout__=K.abs(nout_) # useful nonlinearity   (instead of  RELU)\n",
        "    out_=K.batch_flatten(nout__)  # flatten all ELM \n",
        "    return out_, nout_\n",
        "\n",
        "#================ Apply the conv. layers ==============================\n",
        "inp_chan=np.shape(x_train)[3]\n",
        "print('Number of input channels in image:', inp_chan)\n",
        "\n",
        "# Construct model (binary random kernels) \n",
        "# Layer 1 kernels (without bias)\n",
        "ker=np.sign(np.random.rand(ksize1,ksize1,inp_chan,filtre1).astype('float32')-0.5)\n",
        "#ker=(np.random.rand(ksize1,ksize1,inp_chan,filtre1).astype('float32')-0.5)\n",
        "ker=1*(ker)/(ksize1*ksize1) #scaling \n",
        "\n",
        "if use_stored==1: \n",
        "    ker=bk[0]\n",
        "# Layer 2 kernels (without bias)\n",
        "if typ_con==1:\n",
        "    ker2=np.sign(np.random.rand(ksize2,ksize2,filtre1*inp_chan,filtre2).astype('float32')-0.5)\n",
        "    #ker2=np.random.rand(ksize2,ksize2,filtre1*inp_chan,filtre2).astype('float32')-0.5\n",
        "\n",
        "elif typ_con==2: \n",
        "    ker2=np.sign(np.random.rand(ksize2,ksize2,filtre1,filtre2).astype('float32')-0.5)\n",
        "    # scaling \n",
        "    #ker2=1*(ker2)/(ksize2*ksize2)\n",
        "    #ker2=bker2\n",
        "    #There are 4 dimensions  (a,b,c,d) a,b dim. kernel ; \n",
        "    # c -numeber of channels (1 or 3 for the input, more for 2'nd layer) \n",
        "    # d is the number of  filters opened by this convolution layer .. \n",
        "if use_stored==1:\n",
        "    ker2=bk[1]    \n",
        "# Implementing the processing flow for (train / test) \n",
        "intrain=K.variable(x_train)\n",
        "intest=K.variable(x_test)\n",
        "print(intrain.shape)\n",
        "ker_=K.variable(ker)\n",
        "ker2_=K.variable(ker2)\n",
        "\n",
        "out_train1_, nout_tr=convlayer(intrain,ker_,pool,strp1,strc1,typ_con,pad,pmode)\n",
        "out_test1_, nout_ts=convlayer(intest,ker_,pool,strp1,strc1,typ_con,pad,pmode)\n",
        "# Note: out_ are \"flatten\" outputs,  nout_ are structured outputs  \n",
        "print('Layer 1:',np.shape(nout_tr))\n",
        "# Plus an additional conv layer  \n",
        "if filtre2>0: \n",
        "    out_train_, nout_tr2=convlayer(nout_tr,ker2_,pool2,strp2,strc2,typ_con,pad,pmode)\n",
        "    out_test_, nout_ts2=convlayer(nout_ts,ker2_,pool2,strp2,strc2,typ_con,pad,pmode) \n",
        "    print('Layer 2:',np.shape(nout_tr2))\n",
        "else:\n",
        "    out_train_=out_train1_\n",
        "    out_test_=out_test1_\n",
        "    \n",
        "# Effective computation of the input preprocessing flow \n",
        "t1=ti.time()\n",
        "out_train=K.eval(out_train_)\n",
        "out_test=K.eval(out_test_)\n",
        "t2=ti.time()\n",
        "K.clear_session()  # to avoid overloads \n",
        "\n",
        "# Number of parameters \n",
        "\n",
        "\n",
        "print('Convolutional layers processing time (train + test): ',t2-t1)\n",
        "print('Number of bits in KER1:',np.size(ker))\n",
        "print('Number of bits in KER2:',np.size(ker2))\n",
        "print('Output structure:  ',np.shape(out_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset is: emnist\n",
            "Convolution type is (1-depthwise, 2-classic) 2\n",
            "Layer 1: filters: 10  kernel size: 3 pooling size: (2, 2) strides pooling: 2\n",
            "Layer 1: filters: 16  kernel size: 3 pooling size: (3, 3) strides pooling: 2\n",
            "Pooling mode is: max   Padding is:  same\n",
            "BCONV-ELM - emnist(112800, 28, 28, 1)\n",
            "Number of input channels in image: 1\n",
            "(112800, 28, 28, 1)\n",
            "Layer 1: (112800, 14, 14, 10)\n",
            "Layer 2: (112800, 7, 7, 16)\n",
            "Convolutional layers processing time (train + test):  0.14508819580078125\n",
            "Number of bits in KER1: 90\n",
            "Number of bits in KER2: 1440\n",
            "Output structure:   (112800, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA_u0zGi27UB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9d29dc-46b1-4fb7-ba23-412b058f5426"
      },
      "source": [
        "'''\n",
        "#  ELM MODULE  \n",
        "#================================================================================\n",
        "#  Due to limited GPU RAM - input size should be smaller than 5000 on Kaggle \n",
        "#  or arrange to have less input samples (e.g. 10000 input samples)\n",
        "#  On other platforms it depends on the available resources. \n",
        "#  For larger inputs size is better to use the latest cell (implementing a\n",
        "#  typical multilayer perceptron  MLP in Keras)\n",
        "# Copyright Radu & Ioana DOGARU - radu_d@ieee.org \n",
        "# More details in paper \n",
        "# R. Dogaru and Ioana Dogaru, \"BCONV-ELM: Binary Weights Convolutional \n",
        "# Neural Network Simulator based on Keras/Tensorflow for Low Complexity \n",
        "# Implementations\", in Proceedings ISEEE 2019, in Press. \n",
        "# Please cite the above paper if you find this code useful \n",
        "# \n",
        "#--------------------------------------------------------------------------\n",
        "'''\n",
        "import time as ti\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from extra_keras_datasets import emnist\n",
        "import numpy as np\n",
        "\n",
        "#pentru asta trebuie sa am in y nvaloarea clasei\n",
        "\n",
        "#reduced = 0\n",
        "#(x_train, y_train), (x_test, y_test) = emnist.load_data(type='balanced')\n",
        "print(x_train.shape)\n",
        "#if reduced>0:\n",
        "    #Ntr1=reduced\n",
        "    #x_train=x_train[0:Ntr1,:,:,:] - profu\n",
        "    #x_train=x_train[0:Ntr1,:,:]\n",
        "    #y_train=y_train[0:Ntr1]\n",
        "\n",
        "nr_neuroni= 20000 # Proposed number of neurons on the hidden layer (va folosi modulul BCONV si alt numar pt elm)\n",
        "C=0.001 # Regularization coefficient C  (small value / useful for 0 neurons)\n",
        "if nr_neuroni==0:\n",
        "    C=0.1 # only for no-hidden layer \n",
        "tip=3 # Nonlinearity of the hidden layer (-1 means linear layer)\n",
        "if nr_neuroni==0:\n",
        "    tip=-1   # \n",
        "nb_in=2;  # 0 = float; x - represents weights on a finite x number of bits \n",
        "nb_out=8; # same as above but for the output layer\n",
        "orig=1 # 0 - works with (out_train out_test) from BCONV; \n",
        "       # 1 works with original data - no conv. layers (x_train x_test)\n",
        "       # 0 - convolved data ; 1 - directly applied to ELM \n",
        "#===============  TRAIN DATASET LOADING ==========================================\n",
        "\n",
        "def hidden(x_,inw_,tip):\n",
        "# Hidden layer definit ca \"flow\" Keras (argumentele sunt \"variables\")\n",
        "      hin_=K.dot(inw_,x_)\n",
        "      #----------  HIDDEN LAYER --------- \n",
        "      if tip==-1:  # liniar (Adaline only)\n",
        "        h_=hin_\n",
        "      elif tip==0: # tanh\n",
        "        h_=K.tanh(hin_)\n",
        "      elif tip==1:  # linsat \n",
        "        h_=K.abs(1+hin_)-K.abs(1-hin_)\n",
        "      elif tip==2: # ReLU\n",
        "        h_=K.relu(hin_)\n",
        "      elif tip==3: \n",
        "            h_=K.abs(hin_)\n",
        "      elif tip==4:\n",
        "            h_=K.sqrt(K.square(hin_)+1)\n",
        "      #------------------------------------ \n",
        "      return h_\n",
        "\n",
        "# implements the ELM training procedure with weight quantization       \n",
        "def elmTrain_fix( X, Y, h_Neurons, C , tip, ni):\n",
        "# Training phase - emulated fixed point precision (ni bit quantization)\n",
        "# X - Samples (feature vectors) Y - Labels\n",
        "# ni - number of bits to quantize the inW weights \n",
        "      Ntr = np.size(X,1)\n",
        "      in_Neurons = np.size(X,0)\n",
        "      classes = np.max(Y)\n",
        "      # transforms label into binary columns  \n",
        "      targets = np.zeros( (classes, Ntr), dtype='int8' )\n",
        "      for i in range(0,Ntr):\n",
        "          targets[Y[i]-1, i ] = 1\n",
        "      targets = targets * 2 - 1\n",
        "      \n",
        "      #   Generare inW \n",
        "      #   Generate inW layer \n",
        "      #   Takes care if h_Neurons==0 \n",
        "      if h_Neurons==0:\n",
        "          inW=np.eye(in_Neurons)\n",
        "          h_Neurons=in_Neurons\n",
        "    \n",
        "      else: \n",
        "          rnd = np.random.RandomState()\n",
        "          inW=-1+2*rnd.rand(h_Neurons, in_Neurons).astype('float32')\n",
        "          #inW=rnd.randn(nHiddenNeurons, nInputNeurons).astype('float32')\n",
        "          if ni>0:\n",
        "            Qi=-1+pow(2,ni-1) \n",
        "            inW=np.round(inW*Qi)\n",
        "      \n",
        "      #  Compute hidden layer \n",
        "      iw_=K.variable(inW)\n",
        "      x_=K.variable(X)\n",
        "      h_=hidden(x_,iw_,tip)  \n",
        "      #------------------------------------      \n",
        "      # Moore - Penrose computation of output weights (outW) layer \n",
        "      ta_=K.variable(targets)\n",
        "      print('KERAS ACTIVE')\n",
        "      if h_Neurons<Ntr:\n",
        "          print('LLL - Less neurons than training samples')\n",
        "          outw_=tf.linalg.solve(K.eye(h_Neurons)/C+K.dot(h_,K.transpose(h_)),K.dot(h_,K.transpose(ta_)))  \n",
        "      else:\n",
        "          print('MMM - More neurons than training samples')\n",
        "          outw_=K.dot(h_,tf.linalg.solve(K.eye(Ntr)/C+K.dot(K.transpose(h_),h_),K.transpose(ta_)))\n",
        "      outW=K.eval(outw_)   \n",
        "      K.clear_session()     \n",
        "      return inW, outW \n",
        "      \n",
        "\n",
        "def elmPredict_optim( X, inW, outW, tip):\n",
        "# implements the ELM predictor given the model as arguments \n",
        "# model is simply given by inW, outW and tip \n",
        "# returns a score matrix (winner class has the maximal score)\n",
        "      x_=K.variable(X)\n",
        "      iw_=K.variable(inW)\n",
        "      ow_=K.variable(outW)\n",
        "      h_=hidden(x_,iw_,tip) \n",
        "      mul1=K.dot(K.transpose(h_),ow_)\n",
        "      sc_=K.transpose(mul1)\n",
        "      score = K.eval(sc_)\n",
        "      K.clear_session() \n",
        "      return score \n",
        "        \n",
        "\n",
        "#===============  TRAIN DATASET LOADING ==========================================\n",
        "# converts out_train, y_train into Samples Labels \n",
        "\n",
        "if orig==1:\n",
        "    intrain=K.variable(x_train)\n",
        "    Samples_=K.batch_flatten(intrain)  # aici se aplica direct datele de intrare \n",
        "    Samples=(K.eval(Samples_)).T\n",
        "    print(Samples.shape)\n",
        "else:\n",
        "    Samples=out_train.T \n",
        "    print(\"BCONV - \" + str(Samples.shape))\n",
        "\n",
        "\n",
        "#clase=y_train.shape[1]\n",
        "Labels=(y_train.T+1).astype('int8')\n",
        "\n",
        "if (np.ndim(Labels)<2): \n",
        "    Labels=np.reshape(Labels,[1,np.shape(Labels)[0]])\n",
        "clase=np.max(Labels)\n",
        "print('nr de clase ' + str(clase))\n",
        "#================= TRAIN ELM =====================================================\n",
        "t1 = ti.time()\n",
        "inW, outW = elmTrain_fix(Samples, np.transpose(Labels), nr_neuroni, C, tip, nb_in)\n",
        "trun = ti.time()-t1\n",
        "print(\" training time: %f seconds\" %trun)\n",
        "\n",
        "# ==============  Quantify the output layer ======================================\n",
        "Qout=-1+pow(2,nb_out-1)\n",
        "if nb_out>0:\n",
        "     O=np.max(np.abs(outW))\n",
        "     outW=np.round(outW*(1/O)*Qout)\n",
        "\n",
        "#================= TEST (VALIDATION) DATASET LOADING \n",
        "\n",
        "\n",
        "if orig==1:\n",
        "    intest=K.variable(x_test)\n",
        "    Samples_=K.batch_flatten(intest)  # aici se aplica direct datele de intrare \n",
        "    Samples=(K.eval(Samples_)).T\n",
        "else: \n",
        "    Samples=out_test.T \n",
        "\n",
        "Labels=(y_test.T+1).astype('int8')\n",
        "if (np.ndim(Labels)<2):\n",
        "    Labels=np.reshape(Labels,[1,np.shape(Labels)[0]])  # acopera cazul MNIST \n",
        "\n",
        "n=Samples.shape[0]\n",
        "N=Samples.shape[1]\n",
        "\n",
        "#====================== VALIDATION PHASE (+ Accuracy evaluation) =================\n",
        "t1 = ti.time()\n",
        "scores = elmPredict_optim(Samples, inW, outW, tip)\n",
        "trun = ti.time()-t1\n",
        "print( \" prediction time: %f seconds\" %trun)\n",
        "\n",
        "# CONFUSION MATRIX computation ==================================\n",
        "Conf=np.zeros((clase,clase),dtype='int16')\n",
        "for i in range(N):\n",
        "    # gasire pozitie clasa prezisa \n",
        "    ix=np.where(scores[:,i]==np.max(scores[:,i]))\n",
        "    ixx=np.array(ix)\n",
        "    pred=int(ixx[0,0])\n",
        "    actual=Labels[0,i]-1\n",
        "    Conf[actual,pred]+=1\n",
        "accuracy=100.0*np.sum(np.diag(Conf))/np.sum(np.sum(Conf))\n",
        "print(\"Confusion matrix is: \")\n",
        "print(Conf)\n",
        "print(\"Accuracy is: %f\" %accuracy)\n",
        "print( \"Number of hidden neurons: %d\" %nr_neuroni)\n",
        "print( \"Hidden nonlinearity (0=sigmoid; 1=linsat; 2=Relu; 3 - ABS; 4- multiquadric): %d\" %tip)\n",
        "K.clear_session() \n",
        "#====================================================================================   \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 28, 28, 1)\n",
            "(784, 50000)\n",
            "nr de clase 47\n",
            "KERAS ACTIVE\n",
            "LLL - Less neurons than training samples\n",
            " training time: 18.159588 seconds\n",
            " prediction time: 0.234889 seconds\n",
            "Confusion matrix is: \n",
            "[[268   0   0 ...   1   0   0]\n",
            " [  0 279   1 ...   1   0   0]\n",
            " [  2   0 310 ...   3   0   0]\n",
            " ...\n",
            " [  1   0   2 ... 177   0   2]\n",
            " [  0   0   0 ...   0 369   1]\n",
            " [  1   3   1 ...   1   8 313]]\n",
            "Accuracy is: 79.563830\n",
            "Number of hidden neurons: 20000\n",
            "Hidden nonlinearity (0=sigmoid; 1=linsat; 2=Relu; 3 - ABS; 4- multiquadric): 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elEyUcF-2-4O"
      },
      "source": [
        "# -----------------------------------------------------------------------\n",
        "# KERAS MLP (Up to 2 hidden layer - 0 hidden layers (MLP0)= Adaline \n",
        "#  is recommended for low cpx.) \n",
        "#  Slower than  ELM0, but gives better accuracies\n",
        "#  Fixed point quantization of the MLP0 is implemented in the last cell !\n",
        "#--------------------------------------------------------\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import RMSprop, SGD, Adadelta, Adam, Nadam\n",
        "from tensorflow.keras import utils as np_utils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_size = 100  #  (small values - larger training times but better accuraccy ) \n",
        "epoci = 100 # Number of epochs \n",
        "train_style = 2 # 1-the Keras usual in examples ; 2 keeps best result during training in all epochs (recommended)\n",
        "#--------------  structura model neuronal --------------------------------------------------------\n",
        "nescalat =0  # 0 for scaled data (1 additional scalling) \n",
        "nhid1 = 0 # hidden1 - take 0 for MLP0 (Adaline) - if > 0 nhid2 may also be used \n",
        "nhid2 = 0 # hidden2 - take 0 for MLP0 (or other value if nhid1>0)\n",
        "drop1= 0.0 # droput if drop1>0\n",
        "# Liniar output layer (nhid1=nhid2=0)\n",
        "# ------------------- Optimizer -----------------------------------------------------------------\n",
        "#myopt=SGD(lr=0.01)\n",
        "#myopt = SGD(lr=.007, decay=1e-6, momentum=.9, nesterov=True)\n",
        "myopt =Adadelta(learning_rate=0.1)  # implicit are lr=1 # cum influenteaza valoarea procesul de antrenare ?? \n",
        "#myopt = RMSprop(lr=0.01) \n",
        "#myopt = Nadam()\n",
        "#myopt=Adam()\n",
        "\n",
        "# -------------------------- Loss choice ------------------------------------\n",
        "#my_loss='mean_squared_error'  # desigur se pot alege si alte versiuni \n",
        "#my_loss='mean_absolute_error'\n",
        "my_loss='categorical_crossentropy'\n",
        "\n",
        "# ---------------  conversie  categorical (vector coloana cu 1 pe linia asociata clasei) \n",
        "#num_classes = 1+np.max(y_test).astype('int8')\n",
        "#num_inputs = np.shape(out_test)[1] #- asta e pt retele clasice cnn\n",
        "#num_inputs = 1 #- pt elm si bconv\n",
        "print('Number of inputs: ',num_inputs)\n",
        "print(\"y_test shape: \" + str(y_test.shape))\n",
        "print(\"out_test shape: \" + str(out_test.shape))\n",
        "#yc_train = np_utils.to_categorical(y_train, num_classes)\n",
        "#yc_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "#------------  MODEL Definition  ----------------------------------------\n",
        "# Nota: se poate construi propriul model (vezi https://gist.github.com/abhaikollara/430c0491c851cf0b05a852f1faa805d7  )\n",
        "# ----------------------------------------------\n",
        "model = Sequential()\n",
        "if nhid1>0:\n",
        "    model.add(Dense(nhid1, activation='relu', input_shape=(num_inputs,)))\n",
        "    if drop1>0: \n",
        "        model.add(Dropout(drop1))\n",
        "# second hidden layer \n",
        "if nhid2>0:\n",
        "    model.add(Dense(nhid2, activation='relu'))\n",
        "#   model.add(Dropout(0.2))\n",
        "# output layer \n",
        "if (nhid1+nhid2)==0:\n",
        "    model.add(Dense(num_classes, activation='softmax',input_shape=(num_inputs,)))\n",
        "else: \n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "# --- MODEL COMPILE ---------------------------------------\n",
        "\n",
        "model.compile(loss=my_loss, \n",
        "              optimizer=myopt,   \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# --- RUNS THE MODEL \n",
        "#----\n",
        "model.summary()\n",
        "\n",
        "err_test=np.zeros(epoci)   # For plot   \n",
        "best_acc=0.0\n",
        "best_ep=0\n",
        "t1=ti.time()\n",
        "for k in range(epoci):\n",
        "    model.fit(out_train, yc_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=1,\n",
        "              verbose=0,  #  0 (no details) 1 (details) 2(only epochs)\n",
        "              validation_data=(out_test, yc_test))\n",
        "    score = model.evaluate(out_test, yc_test, verbose=0)\n",
        "    err_test[k]=score[1]\n",
        "    print('Epoch:',k,' ACC:',100*score[1],'| ')\n",
        "    if score[1]>best_acc : \n",
        "            print('improved in epoch:', k, 'best accuracy so far: ', 100*score[1],'%')\n",
        "            best_acc=score[1]\n",
        "            best_ep=k\n",
        "            best_pars=model.get_weights()\n",
        "t2=ti.time()\n",
        "print('Best accuracy:', best_acc*100, '% obtained in epoch: ',best_ep, ' running  ',epoci,' epochs in ',t2-t1,' seconds')\n",
        "t1=ti.time()\n",
        "score = model.evaluate(out_test, yc_test, verbose=0); \n",
        "t2=ti.time()\n",
        "print('Time for prediction (test set):', t2-t1)\n",
        "plt.plot(err_test)\n",
        "print('If quantization is desired see the next cell')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBEV-cuR3CcU"
      },
      "source": [
        "# QMLP0 (quantified MLP0 module)\n",
        "# Quantization of the above resulted model (only for MLP0)\n",
        "# Copyright Radu and Ioana DOGARU;  radu.dogaru@ieee.org\n",
        "#=============================================================\n",
        "nb_out=8\n",
        "outW=np.copy(best_pars)\n",
        "Qout=-1+pow(2,nb_out-1)\n",
        "if (nb_out >0) & (nhid1==0) :\n",
        "    O=np.max(np.abs(outW[0]))\n",
        "    outW[0]=np.round(outW[0]*(1/O)*Qout)\n",
        "    outW[1]=np.round(outW[1]*(1/O)*Qout)\n",
        "    model.set_weights(outW)\n",
        "    score = model.evaluate(out_test, y_test, verbose=0)\n",
        "    best_acc=score[1]\n",
        "    print('Output layer quantized with:', nb_out, 'bits')\n",
        "    print('Quantified accuracy is:', best_acc*100,'%')\n",
        "outW=model.get_weights() # the resulting model \n",
        "Bconvmodel=(ker,ker2,outW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134kLEjHzEMO",
        "outputId": "b92f48bb-8d21-4037-8193-b2a24d154134"
      },
      "source": [
        "#Choose model\n",
        "\n",
        "model_name='resnet50'\n",
        "\n",
        "if model_name=='nl_cnn':\n",
        "  print(\"Generating model \" + model_name)\n",
        "  model = create_nl_cnn_model(input_shape, num_classes, k=2, separ=0, flat=0, width=50, nl=(2,2), add_layer=1)\n",
        "  model.summary()\n",
        "elif model_name=='lb_cnn':\n",
        "  model = create_lb_cnn_model(input_shape, num_classes)\n",
        "  model.summary()\n",
        "  print(\"Generating model \" + model_name)\n",
        "elif model_name=='mobilenetv2':\n",
        "  model = create_model_mobilenetv2(input_shape, num_classes, alpha_value=0.75)\n",
        "  model.summary()\n",
        "  print(\"Generating model \" + model_name)\n",
        "elif model_name=='nasnetmobile50':\n",
        "  model = create_model_nasnetmobile50(input_shape, num_classes)\n",
        "  model.summary()\n",
        "  print(\"Generating model \" + model_name)\n",
        "elif model_name=='resnet50':\n",
        "  model = create_resnet50_model(input_shape, num_classes)\n",
        "  model.summary()\n",
        "  print(\"Generating model \" + model_name)\n",
        "elif model_name=='densenet121':\n",
        "  model = create_densenet121_model(input_shape, num_classes)\n",
        "  model.summary()\n",
        "  print(\"Generating model \" + model_name)\n",
        "elif model_name=='effnetb0':\n",
        "  model = create_effnetb0_model(input_shape, num_classes)\n",
        "  model.summary()\n",
        "  print(\"Generating model \" + model_name)\n",
        "elif model_name=='effnetb1':\n",
        "  model = create_effnetb1_model(input_shape, num_classes)\n",
        "  model.summary()\n",
        "  print(\"Generating model \" + model_name) \n",
        "    "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 1, 1, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_7 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 43)                88107     \n",
            "=================================================================\n",
            "Total params: 23,675,819\n",
            "Trainable params: 23,622,699\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Generating model resnet50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "ZN-O7ccy6nSe",
        "outputId": "d09bebe8-fa3b-4fc1-ae7d-92db18e5252d"
      },
      "source": [
        "# TRAINING OF THE NL-CNN, LB-CNN and predefinied networks  \n",
        "#-----------------  for reproductibility  ----------------------\n",
        "import tensorflow as tf \n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "tf.random.set_seed(2)\n",
        "#----------------------------------------------------\n",
        "\n",
        "import keras\n",
        "import numpy as np # linear algebra\n",
        "import keras.backend as K \n",
        "\n",
        "import time as ti \n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#=====================================================================\n",
        "batch_size = 100  # Ranging betwee 10 (small datasets) to 100 (larger datasets)\n",
        "epoci = 10 # maximal number of training epochs (the best result may be obtained earlier)\n",
        "#----------------------------------------------------------------------------------------\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "err_test=np.zeros(epoci)   # For plotting test error evolution  \n",
        "best_acc=0.0\n",
        "best_ep=0\n",
        "t1=ti.time()\n",
        "for k in range(epoci):\n",
        "      tx=ti.time()\n",
        "      model.fit(x_train, yc_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=1,\n",
        "              verbose=0,  # aici 0 (nu afiseaza nimic) 1 (detaliat) 2(numai epocile)\n",
        "              validation_data=(x_test, yc_test))\n",
        "      \n",
        "      ty=ti.time()\n",
        "      print('/',k,'epoch lasted ',ty-tx,' seconds')\n",
        "      \n",
        "      score = model.evaluate(x_test, yc_test, verbose=0)\n",
        "      \n",
        "      err_test[k]=score[1]\n",
        "      if score[1]>best_acc : \n",
        "            print('Improved in epoch:', k, ' New accuracy: ', 100*score[1],'%')\n",
        "            best_acc=score[1]\n",
        "            best_ep=k\n",
        "            bp=model.get_weights()\n",
        "t2=ti.time()\n",
        "print('Best accuracy:', best_acc*100, '% reached in epoch: ',best_ep, ' running  ',epoci,' epochs lasts ',t2-t1,' seconds')\n",
        "plt.plot(err_test)\n",
        "model.set_weights((bp)) # evaluete prediction time on all test samples\n",
        "t1=ti.time()\n",
        "\n",
        "score = model.evaluate(x_test, yc_test, verbose=0)\n",
        "\n",
        "t2=ti.time()\n",
        "print ('Total number of parameters: ',model.count_params())\n",
        "print('Test accuracy:', score[1])\n",
        "print ('Time to predict on the test set : ',t2-t1)\n",
        "print('Latency (per input sample):', 1000*(t2-t1)/np.shape(x_test)[0], 'ms')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12630, 32, 32, 3)\n",
            "(12630,)\n",
            "/ 0 epoch lasted  31.585200548171997  seconds\n",
            "Improved in epoch: 0  New accuracy:  1.6627078875899315 %\n",
            "/ 1 epoch lasted  40.982667684555054  seconds\n",
            "Improved in epoch: 1  New accuracy:  13.586698472499847 %\n",
            "/ 2 epoch lasted  24.89275813102722  seconds\n",
            "Improved in epoch: 2  New accuracy:  92.16151833534241 %\n",
            "/ 3 epoch lasted  25.310256719589233  seconds\n",
            "Improved in epoch: 3  New accuracy:  94.44972276687622 %\n",
            "/ 4 epoch lasted  24.237404823303223  seconds\n",
            "/ 5 epoch lasted  40.98401236534119  seconds\n",
            "/ 6 epoch lasted  40.983193159103394  seconds\n",
            "/ 7 epoch lasted  40.98431992530823  seconds\n",
            "/ 8 epoch lasted  40.986207008361816  seconds\n",
            "/ 9 epoch lasted  24.637237787246704  seconds\n",
            "Best accuracy: 94.44972276687622 % reached in epoch:  3  running   10  epochs lasts  386.0570251941681  seconds\n",
            "Total number of parameters:  23675819\n",
            "Test accuracy: 0.9444972276687622\n",
            "Time to predict on the test set :  4.741960287094116\n",
            "Latency (per input sample): 0.37545212091006464 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrdWIEpT7IEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09612814-fe67-478a-c1dc-787c937eb4bf"
      },
      "source": [
        "# SAVING AND VISUALIZATION OF A MODEL  \n",
        "#-----------------------------------------------------------\n",
        "nume_dorit = 'resnet_gtsrb_94_44'\n",
        "file_path = '/content/gdrive/MyDrive/datasets/modele/' + nume_dorit\n",
        "model.save(file_path+'.h5')  \n",
        "print('Model ' + nume_dorit + ' saved!')\n",
        "\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "#plot_model(model, to_file=nume_dorit+'.png', show_shapes=True, show_layer_names=True, dpi=96)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model resnet_gtsrb_94_44 saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Z0x2Nq1AFX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu-lVpbiYMzC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "a6d3ca96-9ddb-4d93-a610-1c179d568d22"
      },
      "source": [
        "# Predicting labels  \n",
        "\n",
        "selectia=int(np.shape(x_test)[0]*np.random.rand(1))  # \n",
        "\n",
        "img1=x_test[selectia,:,:,:]\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "if np.shape(x_train)[3]==1:\n",
        "  plt.imshow(img1[:,:,0],cmap='gray')\n",
        "elif np.shape(x_train)[3]==3:\n",
        "  plt.imshow(img1)\n",
        "\n",
        "label=np.dot(yc_test[selectia,:],1+np.array(range(num_classes)).T)\n",
        "print('Original label:', label)\n",
        "z=model.predict(x_test[selectia:selectia+1,:,:,:])\n",
        "print('Recognized label: ',1+np.argmax(z))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original label: 39.0\n",
            "Recognized label:  39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boCuKltrYRTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92f08ec-a4b6-4052-dfd0-348c248b6cf3"
      },
      "source": [
        "from keras.models import load_model\n",
        "model=load_model('/content/gdrive/MyDrive/datasets/modele/resnet_gtsrb_94_44.h5')\n",
        "model.summary()\n",
        "t1=ti.time()\n",
        "score = model.evaluate(x_test, yc_test, verbose=0)\n",
        "t2=ti.time()\n",
        "print('Test accuracy:', score[1])\n",
        "print ('Time for test set : ',t2-t1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 1, 1, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_7 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 43)                88107     \n",
            "=================================================================\n",
            "Total params: 23,675,819\n",
            "Trainable params: 23,622,699\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Test accuracy: 0.9444972276687622\n",
            "Time for test set :  9.065977096557617\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}